{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import beta\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "import itertools as it\n",
    "\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A post about visualization.\n",
    "\n",
    "In previous posts we began using kernel density estimates to classify our haplotypes. We began with a simple classificaiton scenario, but we also saw how KDE could also be used to correct cases of mislabelling.\n",
    "\n",
    "In each case the approach consists of using the density, in feature space, of our reference observations to decide on an ultimate classification. \n",
    "\n",
    "We also made use of a lower threshold to the normalized estimates of these densities to classify haplotypes as outliers.\n",
    "\n",
    "The notion of density of target groups in a shared space will come handy in later posts as well, and i find helpful to visualize all this. \n",
    "\n",
    "This is also an opportunity to play around with plotly and jupyter, which should be fun.\n",
    "\n",
    "Let's skip the intermediate visualizations and jump to it. This block:\n",
    "- simulates reference and unknown haplotyes.\n",
    "- mislables some reference haplotypes.\n",
    "- performs PCA using the whole data set.\n",
    "- extracts normalized KDE scores for each group of labelled individuals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained:\n",
      "PC1: 0.167; PC2: 0.077; PC3: 0.034\n"
     ]
    }
   ],
   "source": [
    "## Let's go ahead and define the density function of our allele frequencies. \n",
    "# :: Refer to previous posts if you wish to visualize this density under the beta distriution ::\n",
    "a, b = 1.5, .2\n",
    "\n",
    "\n",
    "# number of populations to generate\n",
    "N_pops= 4\n",
    "\n",
    "# length of haplotypes\n",
    "L= 200\n",
    "\n",
    "# Size of reference populations\n",
    "Sizes= [250,100,300]\n",
    "labels= [0,1,2] # the fourth population is to be our unknown.\n",
    "\n",
    "# number of unlabelled individuals to draw from each population:\n",
    "n_unlab= {\n",
    "    0: 5,\n",
    "    1: 3,\n",
    "    2: 7,\n",
    "    3: 20\n",
    "}\n",
    "\n",
    "# Simulated crosses between distributions. This dictionary is built to indicate from whom a given pop will\n",
    "# receive its mislabelled material and how many haplotypes will have been contributed, in that order:\n",
    "# Across = {target_pop: {contributing_pop: n_contributions}}\n",
    "\n",
    "Across= {\n",
    "    0:{\n",
    "        1: 3,\n",
    "        2: 5,\n",
    "        3: 4\n",
    "    },\n",
    "    1:{\n",
    "        0: 7,\n",
    "        2: 2\n",
    "    },\n",
    "    2:{\n",
    "        0: 15,\n",
    "        1: 4,\n",
    "        3: 3\n",
    "    }\n",
    "}\n",
    "\n",
    "# reference population labels\n",
    "label_vector= np.repeat(np.array([x for x in labels]),Sizes)\n",
    "\n",
    "## save the allelic frequency vectors that will characterize each population:\n",
    "prob_vectors= np.array([beta.rvs(a, b, size=L) for x in range(N_pops)])\n",
    "prob_vectors[prob_vectors > 1]= 1 ## probabilities exceeding 1 are trimmed.\n",
    "\n",
    "## Drawing haplotypes.\n",
    "data= []\n",
    "\n",
    "for k in range(len(labels)):\n",
    "    \n",
    "    probs= prob_vectors[k,:]\n",
    "    \n",
    "    m= Sizes[k] - sum(Across[k].values())\n",
    "    \n",
    "    Haps= [[np.random.choice([1,0],p= [1-probs[x],probs[x]]) for x in range(L)] for acc in range(m)]\n",
    "    data.extend(Haps)\n",
    "    \n",
    "    ## draw introgressed haplotypes using Across:\n",
    "    \n",
    "    for bro in Across[k].keys():\n",
    "        \n",
    "        haps= [[np.random.choice([1,0],p= [1-prob_vectors[bro,:][x],prob_vectors[bro,:][x]]) for x in range(L)] for acc in range(Across[k][bro])]\n",
    "        data.extend(haps)\n",
    "        \n",
    "data= np.array(data)\n",
    "\n",
    "## create incognita haplotypes from both our known distributions as well as a fourth, uncharactrerized population.\n",
    "admixed= {k:[[np.random.choice([1,0],p= [1-prob_vectors[k,:][x],prob_vectors[k,:][x]]) for x in range(L)] for acc in range(n_unlab[k])] for k in n_unlab.keys()}\n",
    "\n",
    "# Principal component analysis of the data generated.\n",
    "## Number of components to retain:\n",
    "n_comp = 3\n",
    "\n",
    "## Perform the PCA on the whole data set so as not to lose variation absent from our references populations.\n",
    "pca = PCA(n_components=n_comp, whiten=False,svd_solver='randomized').fit(np.vstack((data,[y for y in it.chain(*admixed.values())])))\n",
    "features = pca.transform(data)\n",
    "\n",
    "print(\"Variance explained:\")\n",
    "print(\"; \".join(['PC{0}: {1}'.format(x+1,round(pca.explained_variance_ratio_[x],3)) for x in range(n_comp)]))## stacking our data.\n",
    "\n",
    "\n",
    "### pca transform of our unknowns.\n",
    "admx_t= [y for y in it.chain(*admixed.values())] \n",
    "admx_t= np.array(admx_t)\n",
    "admx_t= pca.transform(admx_t)\n",
    "\n",
    "## Labelled and unlabelled in one data frame.\n",
    "global_data= np.vstack((admx_t,features))\n",
    "\n",
    "## setting our lower limit.\n",
    "Outlier_threshold= 1e-4\n",
    "\n",
    "## calculating kernel bandwidth. A proxy of local differentiation allowed for.\n",
    "params = {'bandwidth': np.linspace(np.min(features), np.max(features),20)}\n",
    "grid = GridSearchCV(KernelDensity(algorithm = \"ball_tree\",breadth_first = False), params,verbose=0)\n",
    "\n",
    "## Estimate Kernel Density of each reference population, normalize and extract scores for every accession in data set.\n",
    "Scores= []\n",
    "\n",
    "# retrieving actual labels\n",
    "actual_labels= [max(labels) + 1]*sum(n_unlab.values())\n",
    "actual_labels.extend(label_vector)\n",
    "Scores.append(actual_labels)\n",
    "\n",
    "vectors= []\n",
    "\n",
    "for lab in labels:\n",
    "    Quanted_set= features[[x for x in range(len(label_vector)) if label_vector[x] == lab],:]\n",
    "    \n",
    "    grid.fit(Quanted_set)\n",
    "    \n",
    "    kde = grid.best_estimator_\n",
    "    \n",
    "    P_dist = kde.score_samples(Quanted_set)\n",
    "    Fist = kde.score_samples(global_data)\n",
    "    \n",
    "    ## Normalizing log-likelihood estimates by those of the reference set.\n",
    "    Fist = scipy.stats.norm(np.mean(P_dist),np.std(P_dist)).cdf(Fist)\n",
    "    vectors.append(Fist)\n",
    "\n",
    "\n",
    "# post-classification labels\n",
    "new_labels= np.argmax(np.array(vectors).T,axis= 1)\n",
    "## Identify cases where all individual scores are below our threshold.\n",
    "below_threshold= [n for n in range(len(new_labels)) if np.amax(np.array(vectors).T,axis= 1)[n] < Outlier_threshold]\n",
    "new_labels[below_threshold]= -1\n",
    "Scores.append(new_labels)\n",
    "\n",
    "Scores.extend(vectors)\n",
    "\n",
    "Scores= np.array(Scores).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This code was mostly copied from previous posts, where it will be found separate and maybe easier to read.**\n",
    "\n",
    "This time however, we'll visualize the output of the KDE using plotly's 'jet' color gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfd70b74a4243f6b8ef3e1f8f3b7d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(Dropdown(description='name', options=('pre-processed', 'classed', 'label: 0', 'label: 1', 'label: 2'), value='pre-processed'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.figure_scores>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def figure_scores(name):\n",
    "    pop= Titles.index(name)\n",
    "    if pop > 1:\n",
    "        fig_data= [go.Scatter3d(\n",
    "                x = global_data[:,0],\n",
    "                y = global_data[:,1],\n",
    "                z = global_data[:,2],\n",
    "                type='scatter3d',\n",
    "                mode= \"markers\",\n",
    "                marker= {\n",
    "                    'color': Scores[:,pop],\n",
    "                    'colorbar': go.ColorBar(\n",
    "                        title= 'ColorBar'\n",
    "                    ),\n",
    "                    'colorscale': 'Viridis',\n",
    "                    'line': {'width': 0},\n",
    "                    'size': 4,\n",
    "                    'symbol': 'circle',\n",
    "                  \"opacity\": .8\n",
    "                  }\n",
    "            )]\n",
    "    else:\n",
    "        coords= {y:[x for x in range(len(Scores)) if Scores[x,pop] == y] for y in list(set(Scores[:,pop]))}\n",
    "        fig_data= [go.Scatter3d(\n",
    "                x = global_data[coords[i],0],\n",
    "                y = global_data[coords[i],1],\n",
    "                z = global_data[coords[i],2],\n",
    "                type='scatter3d',\n",
    "                mode= \"markers\",\n",
    "                marker= {\n",
    "                    'line': {'width': 0},\n",
    "                    'size': 4,\n",
    "                    'symbol': 'circle',\n",
    "                  \"opacity\": .8\n",
    "                  }\n",
    "            ) for i in coords.keys()]\n",
    "    layout = go.Layout(\n",
    "        margin=dict(\n",
    "            l=0,\n",
    "            r=0,\n",
    "            b=0,\n",
    "            t=0\n",
    "        ),\n",
    "        title= name\n",
    "    )\n",
    "    fig = go.Figure(data=fig_data, layout=layout)\n",
    "    iplot(fig)\n",
    "\n",
    "Titles= ['pre-processed','classed'] \n",
    "Titles.extend(['label: {}'.format(x) for x in labels])\n",
    "\n",
    "\n",
    "interact(figure_scores, name=Titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have then a visual representation of the the scores we used to class our haplotypes.\n",
    "\n",
    "In this case, the densities measured were those of points all assigned to the same class. \n",
    "\n",
    "- Isolated haplotypes fell into regions of very low density of similarly assigned objects and were trimmed by our threshold (fig. classed).\n",
    "- Haplotypes far from their label neighborhood but closer to another saw their labels switched.\n",
    "\n",
    "\n",
    "We'll finish off with an exercise: So far we have measured the distribution of observations we knew, or assumed, to be similar. However, we did include in our data set observations drawn from a population we had no reference label for. With our labels we could answer the questions _\"Are you part of our populations?\"_ and, if true, _\"Which?\"_. It was not possible from that output to infer the existence of the extra agglomeration of known and unknown haplotypes that stands out so clearly in the scatter plot.\n",
    "\n",
    "We will now use an unsupervised clustering algorithm, MeanShift (Comaniciu & Meer, 2002), to extract clusters of denser observations in feature space.\n",
    "\n",
    "We will treat each of these clusters as we did our reference populations and visualize their density at each coordinate in the global data set.\n",
    "\n",
    "\n",
    "#### Genetic considerations\n",
    "\n",
    "The assumption here isn't so simple that it should go unmentioned: Normalized KDE scores are an attempt to control for the number and distribution of our reference populations in feature space. Everything in the previous sentence is crucial. \n",
    "\n",
    "- As a measure of variation across the entire data set, distances in feature space are dependent on our sampling (McVean G., 2009). \n",
    "- From a philogenetic view point, principlal component analysis of haplotype data will give the same weight to variation across our variables, or snps.\n",
    "\n",
    "_Assuming we have an representative sample of our study object, that loci are mutating at the same rate and are identical by descent, is higher relative density of a group of points an accurate sign of a more recent common ancestor?_\n",
    "\n",
    "\n",
    "This becomes an issue when our populations aren't as neatly structured or differentiated as we've made them here. But let's take it easy for now, and look at some things we can be sure of. One of those is that haplotypes in neat clusters are indeed closely related. How neat? why don't we let the data decide? In our represention of genetic variation, a rough description of variation allowed for could be the distribution of pairwise distances of neighboring points. By drawing the bandwidth of its kernels from this distribution, the MeanShift clustering algorithm provides a neat solution to this problem, extracting clusters of significantly packed observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## estimate the bandwith\n",
    "bandwidth = estimate_bandwidth(global_data, quantile=0.2)\n",
    "\n",
    "## perform MeanShift clustering.\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True, cluster_all=True, min_bin_freq=5)\n",
    "ms.fit(global_data)\n",
    "labels1 = ms.labels_\n",
    "label_select = {y:[x for x in range(len(labels1)) if labels1[x] == y] for y in sorted(list(set(labels1)))}\n",
    "\n",
    "## Extract the KDE of each cluster identified by MS.\n",
    "cluster_profiles= []\n",
    "\n",
    "cluster_names= Titles\n",
    "\n",
    "for lab in label_select.keys():\n",
    "    cluster_names.append('cl_profile: {}'.format(lab))\n",
    "\n",
    "    Quanted_set= global_data[label_select[lab],:]\n",
    "    \n",
    "    grid.fit(Quanted_set)\n",
    "    \n",
    "    kde = grid.best_estimator_\n",
    "    \n",
    "    P_dist = kde.score_samples(Quanted_set)\n",
    "    Fist = kde.score_samples(global_data)\n",
    "    \n",
    "    ## Normalizing log-likelihood estimates by those of the reference set.\n",
    "    Fist = scipy.stats.norm(np.mean(P_dist),np.std(P_dist)).cdf(Fist)\n",
    "    cluster_profiles.append(Fist)\n",
    "\n",
    "cluster_profiles.append(labels1)\n",
    "cluster_names.append('MSclass')\n",
    "\n",
    "cluster_profiles= np.array(cluster_profiles).T\n",
    "\n",
    "profiles= np.hstack((Scores,cluster_profiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a765e04ef34024a3f51b9764bd60da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(Dropdown(description='name', options=('pre-processed', 'classed', 'label: 0', 'label: 1', 'label: 2', 'cl_profile: 0', 'cl_profile: 1', 'cl_profile: 2', 'cl_profile: 3', 'MSclass'), value='pre-processed'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.figure_scores>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## It is impractical that i cannot give this function an extra argument, but maybe i just haven't found the best solution yet..\n",
    "def figure_scores(name):\n",
    "    pop= cluster_names.index(name)\n",
    "    if name not in ['pre-processed','classed','MSclass']:\n",
    "        fig_data= [go.Scatter3d(\n",
    "                x = global_data[:,0],\n",
    "                y = global_data[:,1],\n",
    "                z = global_data[:,2],\n",
    "                type='scatter3d',\n",
    "                mode= \"markers\",\n",
    "                marker= {\n",
    "                    'color': profiles[:,pop],\n",
    "                    'colorbar': go.ColorBar(\n",
    "                        title= 'ColorBar'\n",
    "                    ),\n",
    "                    'colorscale': 'Viridis',\n",
    "                    'line': {'width': 0},\n",
    "                    'size': 4,\n",
    "                    'symbol': 'circle',\n",
    "                  \"opacity\": .8\n",
    "                  }\n",
    "            )]\n",
    "    else:\n",
    "        coords= {y:[x for x in range(len(profiles)) if profiles[x,pop] == y] for y in list(set(profiles[:,pop]))}\n",
    "        fig_data= [go.Scatter3d(\n",
    "                x = global_data[coords[i],0],\n",
    "                y = global_data[coords[i],1],\n",
    "                z = global_data[coords[i],2],\n",
    "                type='scatter3d',\n",
    "                mode= \"markers\",\n",
    "                marker= {\n",
    "                    'line': {'width': 0},\n",
    "                    'size': 4,\n",
    "                    'symbol': 'circle',\n",
    "                  \"opacity\": .8\n",
    "                  }\n",
    "            ) for i in coords.keys()]\n",
    "    layout = go.Layout(\n",
    "        margin=dict(\n",
    "            l=0,\n",
    "            r=0,\n",
    "            b=0,\n",
    "            t=0\n",
    "        ),\n",
    "        title= name\n",
    "    )\n",
    "    fig = go.Figure(data=fig_data, layout=layout)\n",
    "    iplot(fig)\n",
    "\n",
    "#print(label_select)\n",
    "interact(figure_scores, name=cluster_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that MeanShift identified the fourth population alongside the labelled populations.\n",
    "\n",
    "In this case, where all outlier haplotypes actually belonged to that fourth population, the output of MeanShift is actually the same as that of our reference-based KDE classification - compare figures _classed_ and _MSclass_.\n",
    "\n",
    "Notice that the likelihood vectors have recorded in them the necessary information to extract associated observations in this particular data set.\n",
    "\n",
    "In effect, we have transformed our _m x n_ matrix of genetic information into a _m x k_ matrix of associations.\n",
    "\n",
    "In this simple scenario, that information extracts the same classification as our supervised approach, and so one could argue that the information on associations could be extracted as a single vector of length _m_, our labels. But consider the task of comparing these associations across data sets: \n",
    "- We have no immediate way of knowing which observations were assigned to which labels.\n",
    "- The comparison of classification vectors would have us transform each into binary tables of assignment of size m x ki, ki being the number of cluster identifyed at the ith dataset.\n",
    "- Since we're stuck to that shape, cluster profiles actually offer extra information. Information equals to the added storage space OF float point values relative to integers.\n",
    "- Importantly, and keeping in mind our simultaneous supervised approach, reference populations can be substructered, composed of smaller clusters of more closely related individuals.\n",
    "   \n",
    "In later posts we'll begin wrangling our data to make the most of these density metrics for exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Comaniciu, D., & Meer, P. (2002). Mean shift: A robust approach toward feature space analysis. IEEE Transactions on pattern analysis and machine intelligence, 24(5), 603-619.\n",
    "- McVean, G. (2009). A genealogical interpretation of principal components analysis. PLoS genetics, 5(10), e1000686."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
